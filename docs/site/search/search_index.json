{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"POSHMARK -Assignment Regression Problem Identify/predict/recommend the price for given set of items input : x1-15 output (target) : y_var Data science Steps: - Data Clean - Visualize the data - Ask Questions - Feature Selection based on visualization and try brute force also - Modeling: - Linear Regression - polynomial Regression - Gradient Boosting - Xgboost - Catboost - LightGBM - H20 - Try Formula (if target was not fitted properly) Deployment : You can assume 5000 listings are being created in our platform every minute through our iphone or android app or website. - Scalable solution - should be Horizontally scalable - Use load Balancer - async - docker - RPC call - Use message Queue - KAFKA - Redis - celery - Store the output of the model and data in some Big data systems (HDFS, S3, HIVE) - should not have single point of failure - Analytics as to be done - Feedback based training - Batch analytics Explanation: ### Give explanation for your prediction if needed - lime - SHAP Give confidence interval also for the price Feedback: - Take Feedback about the price recommendation - Use this Feedback to improve/train the models Attach Rest API -Docker Deployment scripts","title":"Home"},{"location":"#poshmark-assignment","text":"","title":"POSHMARK -Assignment"},{"location":"#regression-problem","text":"","title":"Regression Problem"},{"location":"#identifypredictrecommend-the-price-for-given-set-of-items","text":"input : x1-15 output (target) : y_var","title":"Identify/predict/recommend the price for given set of items"},{"location":"#data-science-steps","text":"- Data Clean - Visualize the data - Ask Questions - Feature Selection based on visualization and try brute force also - Modeling: - Linear Regression - polynomial Regression - Gradient Boosting - Xgboost - Catboost - LightGBM - H20 - Try Formula (if target was not fitted properly)","title":"Data science Steps:"},{"location":"#deployment","text":"You can assume 5000 listings are being created in our platform every minute through our iphone or android app or website. - Scalable solution - should be Horizontally scalable - Use load Balancer - async - docker - RPC call - Use message Queue - KAFKA - Redis - celery - Store the output of the model and data in some Big data systems (HDFS, S3, HIVE) - should not have single point of failure - Analytics as to be done - Feedback based training - Batch analytics","title":"Deployment :"},{"location":"#explanation","text":"### Give explanation for your prediction if needed - lime - SHAP","title":"Explanation:"},{"location":"#give-confidence-interval-also-for-the-price","text":"","title":"Give confidence interval also for the price"},{"location":"#feedback","text":"- Take Feedback about the price recommendation - Use this Feedback to improve/train the models","title":"Feedback:"},{"location":"#attach-rest-api","text":"-Docker Deployment scripts","title":"Attach Rest API"},{"location":"deployment/","text":"Deployment of the model APIs since we want low-latency prediction we used async service instead of sync service Here are APIs which can be deployed as part of this assignment Prediction API Feedback API Explanation API confidence Interval API prediction API This is designed as a low latency async api This API expects x1-x15 feature in a json format and returns a price recommendation Response shown in the below figure Feedback API (NotImplemented) Where the feedback about the REcommended Price can be given by the user Explanation (NotImplemented) Explanation for the every prediction why the model recommended particular value as prediction Confidence Interval (NotImplemented) Instead predicting single price , if give exact price and range of the price it will be helpful to the user","title":"Deployment"},{"location":"deployment/#deployment-of-the-model-apis","text":"","title":"Deployment of the model APIs"},{"location":"deployment/#since-we-want-low-latency-prediction-we-used-async-service-instead-of-sync-service","text":"","title":"since we want low-latency prediction  we used async service instead of sync service"},{"location":"deployment/#here-are-apis-which-can-be-deployed-as-part-of-this-assignment","text":"Prediction API Feedback API Explanation API confidence Interval API","title":"Here are APIs which can be deployed as part of this assignment"},{"location":"deployment/#prediction-api","text":"This is designed as a low latency async api This API expects x1-x15 feature in a json format and returns a price recommendation Response shown in the below figure","title":"prediction API"},{"location":"deployment/#feedback-api-notimplemented","text":"Where the feedback about the REcommended Price can be given by the user","title":"Feedback API (NotImplemented)"},{"location":"deployment/#explanation-notimplemented","text":"Explanation for the every prediction why the model recommended particular value as prediction","title":"Explanation (NotImplemented)"},{"location":"deployment/#confidence-interval-notimplemented","text":"Instead predicting single price , if give exact price and range of the price it will be helpful to the user","title":"Confidence Interval (NotImplemented)"},{"location":"eda/","text":"Data cleaning : There is no missing value in the given dataset Exploring the Dataset Visualizing all the columns at once To find how they are correlated with each other and with target (price) variable Visualizing Target variable only y_var Visualizing only continous Columns Visualizing only catagorical columns (value counts): Visualizing Correlation between all the columns Visualizing Correlation for single column Out of curiosity found that majority of the target variable (price) found to be a zero so visualizing the dataFrame which results in zero price","title":"Exploratory data Analysis"},{"location":"eda/#data-cleaning","text":"There is no missing value in the given dataset","title":"Data cleaning :"},{"location":"eda/#exploring-the-dataset","text":"Visualizing all the columns at once To find how they are correlated with each other and with target (price) variable Visualizing Target variable only y_var Visualizing only continous Columns Visualizing only catagorical columns (value counts): Visualizing Correlation between all the columns Visualizing Correlation for single column Out of curiosity found that majority of the target variable (price) found to be a zero so visualizing the dataFrame which results in zero price","title":"Exploring the Dataset"},{"location":"explanation/","text":"Explanation Giving Explanation for predicted /recommended price boost user confidence Global Explanation Whole data is used to get the explanation from the model This explanation is computed based on all the training data (kind of another feature importance) Summary Plot Local Explanation Every single data send through the website can be given a explanation for Example Local Explanation Refer this url below:","title":"Explanation"},{"location":"explanation/#explanation","text":"Giving Explanation for predicted /recommended price boost user confidence","title":"Explanation"},{"location":"explanation/#global-explanation","text":"Whole data is used to get the explanation from the model This explanation is computed based on all the training data (kind of another feature importance) Summary Plot","title":"Global Explanation"},{"location":"explanation/#local-explanation","text":"Every single data send through the website can be given a explanation for Example Local Explanation Refer this url below:","title":"Local Explanation"},{"location":"feat_sel/","text":"Feature Selection Feature imporatance of all baseline models linear_reg lasso ridge rf gbr cat_gbr light_gbr 0 2.23977 1.95428 2.23979 0.0112881 0.000124869 0.742056 26 1 -3.00007 -0 -3 0.0144287 0.00254696 1.45523 27 2 -0.0497457 -0.0584689 -0.0497475 0.0513834 0.00436767 4.42543 196 3 -1.50822 -1.40005 -1.50829 0.0741738 0.163885 13.1443 342 4 -0.638839 -0.614495 -0.63882 0.0734982 0.067889 9.07051 568 5 0.608251 0.609754 0.608252 0.304805 0.481872 30.1835 502 6 -0.000357175 -0.000236782 -0.000357118 0.0627593 0.0435428 6.78723 235 7 158.626 142.846 158.612 0.0771707 0.109547 12.0767 172 8 34.0068 30.371 34.0059 0.0128193 0.0155619 1.03683 6 9 -0.00218529 -0.00183329 -0.00218526 0.0879956 0.0108722 2.54453 190 10 0.17857 0.222073 0.178571 0.121091 0.0553608 5.73852 224 11 46.8198 8.60254 46.8128 0.00558398 0.00522745 5.86023 80 12 -0.00775879 0.0194101 -0.00774927 0.00479453 0.0008099 1.02793 50 13 -15.9664 -15.0783 -15.9672 0.0498253 0.0130437 2.05423 175 14 0.00161799 0.00375283 0.00161861 0.0483828 0.0253482 3.8528 207","title":"Feature Selection"},{"location":"feat_sel/#feature-selection","text":"Feature imporatance of all baseline models linear_reg lasso ridge rf gbr cat_gbr light_gbr 0 2.23977 1.95428 2.23979 0.0112881 0.000124869 0.742056 26 1 -3.00007 -0 -3 0.0144287 0.00254696 1.45523 27 2 -0.0497457 -0.0584689 -0.0497475 0.0513834 0.00436767 4.42543 196 3 -1.50822 -1.40005 -1.50829 0.0741738 0.163885 13.1443 342 4 -0.638839 -0.614495 -0.63882 0.0734982 0.067889 9.07051 568 5 0.608251 0.609754 0.608252 0.304805 0.481872 30.1835 502 6 -0.000357175 -0.000236782 -0.000357118 0.0627593 0.0435428 6.78723 235 7 158.626 142.846 158.612 0.0771707 0.109547 12.0767 172 8 34.0068 30.371 34.0059 0.0128193 0.0155619 1.03683 6 9 -0.00218529 -0.00183329 -0.00218526 0.0879956 0.0108722 2.54453 190 10 0.17857 0.222073 0.178571 0.121091 0.0553608 5.73852 224 11 46.8198 8.60254 46.8128 0.00558398 0.00522745 5.86023 80 12 -0.00775879 0.0194101 -0.00774927 0.00479453 0.0008099 1.02793 50 13 -15.9664 -15.0783 -15.9672 0.0498253 0.0130437 2.05423 175 14 0.00161799 0.00375283 0.00161861 0.0483828 0.0253482 3.8528 207","title":"Feature Selection"},{"location":"hyperameter_tuning/","text":"Hyperameter Tunning We settled with the Catboost model for good Quality prediction and low latency prediction This is the table of hyperparameters and their performance metric ('number', '') ('state', '') ('value', '') ('datetime_start', '') ('datetime_complete', '') ('params', 'bagging_temperature') ('params', 'boosting_type') ('params', 'bootstrap_type') ('params', 'depth') ('params', 'learning_rate') ('params', 'objective') ('params', 'subsample') ('system_attrs', '_number') ('system_attrs', 'fail_reason') 0 0 TrialState.FAIL nan 2020-01-02 20:21:11.863237 2020-01-02 20:21:12.418973 nan nan nan 6 nan RMSE nan 0 Setting status of trial#0 as TrialState.FAIL because of the following error: TypeError('suggest_uniform() takes 4 positional arguments but 8 were given',) 1 1 TrialState.FAIL nan 2020-01-02 20:21:38.181381 2020-01-02 20:21:38.818264 nan nan nan 7 nan MAE nan 1 Setting status of trial#1 as TrialState.FAIL because of the following error: TypeError('suggest_uniform() takes 4 positional arguments but 6 were given',) 2 2 TrialState.FAIL nan 2020-01-02 20:22:14.453568 2020-01-02 20:22:15.072724 nan nan nan 2 nan RMSE nan 2 Setting status of trial#2 as TrialState.FAIL because of the following error: TypeError('suggest_uniform() takes 4 positional arguments but 5 were given',) 3 3 TrialState.COMPLETE 42.7238 2020-01-02 20:22:23.798479 2020-01-02 20:22:39.130715 nan Plain MVS 11 0.489043 MAE nan 3 nan 4 4 TrialState.COMPLETE 50.5916 2020-01-02 20:22:39.215907 2020-01-02 20:22:42.882038 nan Plain Bernoulli 5 0.529281 RMSE 0.607423 4 nan 5 5 TrialState.COMPLETE 42.5857 2020-01-02 20:22:42.963390 2020-01-02 20:23:43.219132 nan Plain Bernoulli 3 0.683597 MAE 0.374904 5 nan 6 6 TrialState.COMPLETE 46.9529 2020-01-02 20:23:43.311247 2020-01-02 20:24:41.338719 nan Plain MVS 1 0.0137888 MAE nan 6 nan 7 7 TrialState.COMPLETE 44.549 2020-01-02 20:24:41.431578 2020-01-02 20:25:29.054821 1.91368 Ordered Bayesian 1 0.384213 MAE nan 7 nan 8 8 TrialState.COMPLETE 41.917 2020-01-02 20:25:29.147049 2020-01-02 20:26:22.714733 nan Ordered MVS 5 0.226351 MAE nan 8 nan 9 9 TrialState.COMPLETE 50.1082 2020-01-02 20:26:22.825344 2020-01-02 20:26:34.244275 nan Plain MVS 12 0.547259 RMSE nan 9 nan 10 10 TrialState.COMPLETE 52.9363 2020-01-02 20:26:34.396108 2020-01-02 20:26:37.911406 nan Plain Bernoulli 5 0.603843 RMSE 0.303779 10 nan 11 11 TrialState.COMPLETE 51.0596 2020-01-02 20:26:38.002327 2020-01-02 20:26:46.647710 8.14127 Ordered Bayesian 4 0.754896 RMSE nan 11 nan 12 12 TrialState.COMPLETE 50.528 2020-01-02 20:26:46.733670 2020-01-02 20:26:51.632994 nan Ordered Bernoulli 4 0.51049 RMSE 0.546119 12 nan 13 13 TrialState.COMPLETE 42.2105 2020-01-02 20:26:51.725349 2020-01-02 20:27:16.891392 nan Ordered MVS 9 0.212695 MAE nan 13 nan 14 14 TrialState.COMPLETE 43.212 2020-01-02 20:27:16.980661 2020-01-02 20:27:50.826629 nan Ordered MVS 9 0.209423 MAE nan 14 nan 15 15 TrialState.COMPLETE 42.605 2020-01-02 20:27:50.911060 2020-01-02 20:28:22.338762 nan Ordered MVS 8 0.171057 MAE nan 15 nan 16 16 TrialState.COMPLETE 42.9026 2020-01-02 20:28:22.445095 2020-01-02 20:28:46.446403 nan Ordered MVS 8 0.29398 MAE nan 16 nan 17 17 TrialState.COMPLETE 42.9598 2020-01-02 20:28:46.558330 2020-01-02 20:30:10.208094 nan Ordered MVS 10 0.0427293 MAE nan 17 nan Apart from the above Optimization, we did some tunning in the modeling phase (using catboost model)","title":"Hyperameter_tuning"},{"location":"hyperameter_tuning/#hyperameter-tunning","text":"We settled with the Catboost model for good Quality prediction and low latency prediction This is the table of hyperparameters and their performance metric ('number', '') ('state', '') ('value', '') ('datetime_start', '') ('datetime_complete', '') ('params', 'bagging_temperature') ('params', 'boosting_type') ('params', 'bootstrap_type') ('params', 'depth') ('params', 'learning_rate') ('params', 'objective') ('params', 'subsample') ('system_attrs', '_number') ('system_attrs', 'fail_reason') 0 0 TrialState.FAIL nan 2020-01-02 20:21:11.863237 2020-01-02 20:21:12.418973 nan nan nan 6 nan RMSE nan 0 Setting status of trial#0 as TrialState.FAIL because of the following error: TypeError('suggest_uniform() takes 4 positional arguments but 8 were given',) 1 1 TrialState.FAIL nan 2020-01-02 20:21:38.181381 2020-01-02 20:21:38.818264 nan nan nan 7 nan MAE nan 1 Setting status of trial#1 as TrialState.FAIL because of the following error: TypeError('suggest_uniform() takes 4 positional arguments but 6 were given',) 2 2 TrialState.FAIL nan 2020-01-02 20:22:14.453568 2020-01-02 20:22:15.072724 nan nan nan 2 nan RMSE nan 2 Setting status of trial#2 as TrialState.FAIL because of the following error: TypeError('suggest_uniform() takes 4 positional arguments but 5 were given',) 3 3 TrialState.COMPLETE 42.7238 2020-01-02 20:22:23.798479 2020-01-02 20:22:39.130715 nan Plain MVS 11 0.489043 MAE nan 3 nan 4 4 TrialState.COMPLETE 50.5916 2020-01-02 20:22:39.215907 2020-01-02 20:22:42.882038 nan Plain Bernoulli 5 0.529281 RMSE 0.607423 4 nan 5 5 TrialState.COMPLETE 42.5857 2020-01-02 20:22:42.963390 2020-01-02 20:23:43.219132 nan Plain Bernoulli 3 0.683597 MAE 0.374904 5 nan 6 6 TrialState.COMPLETE 46.9529 2020-01-02 20:23:43.311247 2020-01-02 20:24:41.338719 nan Plain MVS 1 0.0137888 MAE nan 6 nan 7 7 TrialState.COMPLETE 44.549 2020-01-02 20:24:41.431578 2020-01-02 20:25:29.054821 1.91368 Ordered Bayesian 1 0.384213 MAE nan 7 nan 8 8 TrialState.COMPLETE 41.917 2020-01-02 20:25:29.147049 2020-01-02 20:26:22.714733 nan Ordered MVS 5 0.226351 MAE nan 8 nan 9 9 TrialState.COMPLETE 50.1082 2020-01-02 20:26:22.825344 2020-01-02 20:26:34.244275 nan Plain MVS 12 0.547259 RMSE nan 9 nan 10 10 TrialState.COMPLETE 52.9363 2020-01-02 20:26:34.396108 2020-01-02 20:26:37.911406 nan Plain Bernoulli 5 0.603843 RMSE 0.303779 10 nan 11 11 TrialState.COMPLETE 51.0596 2020-01-02 20:26:38.002327 2020-01-02 20:26:46.647710 8.14127 Ordered Bayesian 4 0.754896 RMSE nan 11 nan 12 12 TrialState.COMPLETE 50.528 2020-01-02 20:26:46.733670 2020-01-02 20:26:51.632994 nan Ordered Bernoulli 4 0.51049 RMSE 0.546119 12 nan 13 13 TrialState.COMPLETE 42.2105 2020-01-02 20:26:51.725349 2020-01-02 20:27:16.891392 nan Ordered MVS 9 0.212695 MAE nan 13 nan 14 14 TrialState.COMPLETE 43.212 2020-01-02 20:27:16.980661 2020-01-02 20:27:50.826629 nan Ordered MVS 9 0.209423 MAE nan 14 nan 15 15 TrialState.COMPLETE 42.605 2020-01-02 20:27:50.911060 2020-01-02 20:28:22.338762 nan Ordered MVS 8 0.171057 MAE nan 15 nan 16 16 TrialState.COMPLETE 42.9026 2020-01-02 20:28:22.445095 2020-01-02 20:28:46.446403 nan Ordered MVS 8 0.29398 MAE nan 16 nan 17 17 TrialState.COMPLETE 42.9598 2020-01-02 20:28:46.558330 2020-01-02 20:30:10.208094 nan Ordered MVS 10 0.0427293 MAE nan 17 nan","title":"Hyperameter Tunning"},{"location":"hyperameter_tuning/#apart-from-the-above-optimization-we-did-some-tunning-in-the-modeling-phase-using-catboost-model","text":"","title":"Apart from the above Optimization, we did some tunning in the modeling phase (using catboost model)"},{"location":"modeling/","text":"Modeling What are the Common price amount always happening for the given list of variables(x-var)? This is useful to get a intiution This can be used in final stage of modeling (we can tweak the model output , to get desired output at the last) Feature Selection using trained models Build Different Algorithms to predict the price Linear, Lasso,Ridge Xgboost,Catboost,LightGBM Pytorch model (if needed) Baseline model performance metrics Measure the model performance using Mean Absolute Error Mean Squared Error RMSE Benchmark Leaderboard of models using Different Hyperparameters For above metrics(Error function in the test set) Also for Prediction Time Confidence Interval Calculation Random Forest Confidence Intervals (using different Decision Trees) Gradient Boosting using Quantile Intervals Explanation for the prediction Shap Lime Developing Baseline Model we trained 7 Models with no hyperparameter tuning as our first baseline: Linear Regression Lasso Regression Ridge Regression RandomForest Regression GradientBoosting Regression (GBR) CatBoost GBR lightGBM GBR Baseline Score of the trained models linear_reg lasso ridge rf gbr cat_gbr light_gbr maae 23.9081 24.9784 23.9086 22.1 23.3951 22.6081 22.4571 mae 52.7526 52.9087 52.7526 55.2584 49.892 49.4214 49.6741 mse 21382.3 21457.7 21382.4 23178.5 20087.1 19066.8 19289.1 rmse 146.227 146.484 146.227 152.245 141.729 138.083 138.885 Among all the models catboost Regressor performed well on the test dataset As we took Catboost model for best prediction accuracy and low-latency predictions and started tuning the hyperparamater of the models The Training notebook of the catboost model was attached as html in the following link --","title":"Modeling"},{"location":"modeling/#modeling","text":"What are the Common price amount always happening for the given list of variables(x-var)? This is useful to get a intiution This can be used in final stage of modeling (we can tweak the model output , to get desired output at the last) Feature Selection using trained models Build Different Algorithms to predict the price Linear, Lasso,Ridge Xgboost,Catboost,LightGBM Pytorch model (if needed) Baseline model performance metrics Measure the model performance using Mean Absolute Error Mean Squared Error RMSE Benchmark Leaderboard of models using Different Hyperparameters For above metrics(Error function in the test set) Also for Prediction Time Confidence Interval Calculation Random Forest Confidence Intervals (using different Decision Trees) Gradient Boosting using Quantile Intervals Explanation for the prediction Shap Lime","title":"Modeling"},{"location":"modeling/#developing-baseline-model","text":"","title":"Developing Baseline Model"},{"location":"modeling/#we-trained-7-models-with-no-hyperparameter-tuning-as-our-first-baseline","text":"Linear Regression Lasso Regression Ridge Regression RandomForest Regression GradientBoosting Regression (GBR) CatBoost GBR lightGBM GBR","title":"we trained 7 Models with no hyperparameter tuning as our first baseline:"},{"location":"modeling/#baseline-score-of-the-trained-models","text":"linear_reg lasso ridge rf gbr cat_gbr light_gbr maae 23.9081 24.9784 23.9086 22.1 23.3951 22.6081 22.4571 mae 52.7526 52.9087 52.7526 55.2584 49.892 49.4214 49.6741 mse 21382.3 21457.7 21382.4 23178.5 20087.1 19066.8 19289.1 rmse 146.227 146.484 146.227 152.245 141.729 138.083 138.885 Among all the models catboost Regressor performed well on the test dataset","title":"Baseline Score of the trained models"},{"location":"modeling/#as-we-took-catboost-model-for-best-prediction-accuracy-and-low-latency-predictions-and-started-tuning-the-hyperparamater-of-the-models","text":"The Training notebook of the catboost model was attached as html in the following link --","title":"As we took Catboost model for best prediction accuracy and low-latency predictions and started tuning the hyperparamater of the models"}]}